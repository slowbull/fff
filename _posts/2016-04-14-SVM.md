---
layout: post
title: "SVM notes"
author: 
modified:
excerpt: "Note of SVM algorithm"
tags: [Machine Learning]
---

###SVM
This is about the derivation of support vector machine.



###Primal problem
Suppose this is a two class classification problem, $y_i \in \{+1,-1\}$. There are $n$ data samples, and $x_i \in R^d$.
In this problem, we assume that it is linear separable, so a linear hyperplane could classify this problem perfectly. We will discuss the case of linear nonseparable later.

Build a function:
$$
f(x) = w^Tx + b
$$
Classification hyperplane:
$$
w^T x + b = 0
$$

for any point outside this plane $x$, there is a point $x_0$ in this plane and,
$$
x = x_0 + r\frac{w}{||w||_2}
$$
where $r$ is the distance of the point $x$ to the hyperplane.

Because $w^Tx_0 + b = 0$, so
$$
f(x) = w^x+b = w^Tx_0 +b +r||w||_2 = r||w||_2
$$
*Geometrical margin *  $r$:
$$
r = \frac{f(x)}{||w||_2}
$$

$r$ can be positive or negative, and it depends on the sign of $f(x)$.

We define *Functional Margin*:
$$
\frac{yf(x)}{||w||_2}
$$

So, we can maximize the smallest margin from different classes to this hyperplane. Then we can  build a problem
$$
\begin{aligned}
\max_w &\ \frac{\tilde{y}f(\tilde{x})}{||w||_2} \\
\textrm{s.t.} & \tilde{y}f(\tilde{x}) \leq y_if(x_i), i \in {1,2,...n}
\end{aligned}
$$
where $\tilde{x}$ is the closest point to this hyperplane.
Define $\tilde{y}f(\tilde{x}) = 1$, this problem becomes,
$$
\begin{aligned}
\max_w &\ \frac{1}{||w||_2} \\
\textrm{s.t.} &  y_if(x_i) \geq 1, i \in {1,2,...n}
\end{aligned}
$$

Minimize $||w||_2$ and $||w||^2$ is equal, and to make things easier, we multiply $\frac{1}{2}$, so, the primal problem of SVM:
$$
\begin{aligned}
\min_w & \ \frac{1}{2} {||w||^2} \\
\textrm{s.t.} &  y_i (w^Tx_i + b) \geq 1, i \in {1,2,...n}
\end{aligned}
$$
Actually, we can solve this problem easily. But if we transform this problem to its dual problem, we will find some interesting phenomenon, and this is where kernel comes out.

###Dual problem
Transform the primal problem to lagragian dual problem.
Lagragian:
$$
\begin{aligned}
L(w,b,\alpha) = \frac{1}{2}||w||^2 + \sum_{i=1}^n \alpha_i (1- y_i(w^Tx_i + b))
\end{aligned}
$$
where $\alpha_i$ is the lagragian multiplier and $\alpha_i \geq 0$.

Because $y_i (w^Tx_i + b) \geq 1$ and $\alpha_i \geq 0$, it is easy to know that it is true. If $y_i (w^Tx_i + b) > 1$, $\alpha_i=0$; if $y_i (w^Tx_i + b) = 1$, $\alpha_i \neq 0$, and this is the support vector. And
$$
\max_\alpha L(w,b,\alpha) = \frac{1}{2}||w||^2
$$

The primal problem
$$
\theta(w,b) = \min_{w,b} \max_{\alpha \geq 0} L(w,b,\alpha)
$$

If we swap the space of min and max, we get the dual problem.
$$
\max_{\alpha \geq 0} \min_{w,b} L(w,b,\alpha) = \max_{\alpha \geq 0} D(\alpha)
$$

Suppose $D^*$ is the maximum of dual function, and $P^*$ is the minimum of primal function.
It is always true that,
$$
D^* \leq P^*
$$
It's easy to understand because the maximum of the minimum is smaller than the minimum of the maximum. This is called weak duality.
The strong duality is:
$$
D^* = P^*
$$
For convex prolem, if it satisfies Slater condition, then it is strong duality. If it is strong duality, it always satisfys KKT conditions. Our problem is strong duality, so we can solve this problem by its dual form.

Dual function:
$$
\begin{aligned}
D(\alpha) = & \inf_{w,b} L(w,b,\alpha)\\
 = & \min_{w,b} \frac{1}{2}||w||^2 + \sum_{i=1}^n \alpha_i (1- y_i(w^Tx_i + b))
\end{aligned}
$$

Take derivative of $D(\alpha)$ on $w$ and $b$ respectively and make them zero.
$$
\begin{aligned}
\frac{\partial L(w,b,\alpha)}{\partial w} &= w + \sum_{i=1}^n-\alpha_i y_i x_i = 0\\
\frac{\partial L(w,b,\alpha)}{\partial b} &= \sum_{i=1}^n -\alpha_i y_i = 0
\end{aligned}
$$
So,
$$
w = \sum_{i=1}^{n}\alpha_i y_i x_i\\
\sum_{i=1}^n \alpha_i y_i = 0
$$
Input the above equations in the dual function.
$$
D(\alpha) =  \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i y_i y_j x_i^T x_j \alpha_j
$$
So, finally, we get the dual problem:
$$
\begin{aligned}
\max_{\alpha} & \   \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i y_i y_j x_i^T x_j \alpha_j \\
\textrm{s.t.} & \ \alpha_i \geq 0, i \in {1,2,...n}\\
&\ \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$

When we calculate $\alpha_i$, the classification hyperplane is,
$$
f(x) = \sum_{i=1}^{n}\alpha_i y_i x_i^T x + b
$$
The inner product of $x_i^T x_j$ in the dual problem and $x_i^Tx$ attract the attention of researchers and this is where kernel coming out.

###Kernel
Feature is very important for model performance, and the procedures to generate features are called feature engineering. Kernel method can be looked as a feature engineering method, at least I think so.

Through dual problem, we know that we just need to compute the inner product of $x_i^T x_j$, this is very nontrivial in kernel method. Cause it is a scalar. We can project $x$ to $\phi(x)$, and we use the transformed feature to train svm, and we just need to compute $\phi(x_i)^T \phi(x_j)$, and it is a scalar too.

Suppose $x = [x_1,x_2]$ and $y = [y_1, y_2]$, and we $\phi(x) = [x_1,x_2,x_1^2,x_2^2,x_1x_2]$, $\phi(y) = [y_1,y_2,y_1^2,y_2^2,y_1y_2]$.
We can know that,
$$
\phi(x)^T\phi(y) = a x_1y_1 + b x_2y_2 + cx_1^2y_1^2 + dx_2^2y_2^2 + ex_1x_2y_1y_2
$$

If we expand $(xy+1)^2$, we will find they have the same form, and the only difference comes from coefficients. So, we don't need to know the form of $\phi(x)$, or we don't need to know how to generate these new features, we just need to calculate this inner product value through a kernel. This is called kernel method.

There are three common kernels.
1. linear kernel. $\phi(x)^T\phi(y) = xy$. It is very powerful, and Liblinear is a fast implementation of linear svm.
2. Polynomial kernel. $\phi(x)^T\phi(y) = (xy+R)^d$
3. Gaussian RBF kernel. $e^{\frac{(x-y)^2}{\sigma^2}}$. It is said to be able to project original feature to infinite dimmension.

So the dual problem above becomes:
$$
\begin{aligned}
\max_{\alpha} & \   \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i y_i y_j K(x_i, x_j) \alpha_j \\
\textrm{s.t.} & \ \alpha_i \geq 0, i \in {1,2,...n}\\
&\ \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$

When we calculate $\alpha_i$, the classification hyperplane is,
$$
f(x) = \sum_{i=1}^{n}\alpha_i y_i K(x_i, x) + b
$$


### Outliers
In the above, we suppose that these samples are separable, however, it is common that there are some outliers, so that it will influnce the model building procedure.

So, we introduce a new variable in SVM model.
$$
\begin{aligned}
\min_w & \ \frac{1}{2} {||w||^2}  + C \sum_{i=1}^{n} \varepsilon_i\\
\textrm{s.t.} &  y_i (w^Tx_i + b) \geq 1 - \varepsilon_i, i \in {1,2,...n} \\
& \varepsilon_i \geq 0
\end{aligned}
$$
where $\varepsilon_i$ is the slack variable, and works as the deviation from the support vector plane. $C$ is the parameter controls the maximum margin and outliers.

Dual function:
$$
\frac{1}{2}||w||^2 + C \sum_{i=1}^n \varepsilon_i + \sum_{i=1}^{n} \alpha_i (1 - \varepsilon_i - y_i(w^Tx_i+b)) - \sum_{i=1}^{n} \beta_i \varepsilon_i
$$

Take derivative of the dual function with respect to $w$, $b$ and $\varepsilon$.
We can derive the dual problem finally.
$$
\begin{aligned}
\max_{\alpha} & \   \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i y_i y_j x_i^T x_j \alpha_j \\
\textrm{s.t.} & \ C \geq \alpha_i \geq 0, i \in {1,2,...n}\\
&\ \sum_{i=1}^n \alpha_i y_i = 0
\end{aligned}
$$

[1] http://blog.pluskid.org/?page_id=683
[2] http://research.microsoft.com/pubs/67119/svmtutorial.pdf
